---
title: "Lab 2, Group B9"
author: "Anton Gefvert, Richard Friberg, Ruben Hillborg"
date: "12/10/2018"
output:
  html_document:
    df_print: paged
  pdf_document:
    fig_caption: yes
---

```{r setup, include=FALSE}
#setup
knitr::opts_chunk$set(echo = FALSE, warning=F)
library(MASS)
library(readxl)
library(knitr)
library(tree)
library(e1071)
library(ROCR)
library(fastICA)

```

# Contributions
* Assignment 1: Anton Gefvert
* Assignment 2: Ruben Hillborg
* Assignment 3: Richard Friberg

# Assignment 1

## 1.1

```{r load_crabs, fig.pos='H', fig.cap="\\label{fig:crabs} Carapace Length vs Rear Width classified by sex", fig.width=5, fig.height=4}
#1.1
data1 = read.csv("australian-crabs.csv")

male_data = data1[data1$sex == "Male",]
female_data = data1[data1$sex == "Female",]

plot(data1$CL, data1$RW, ylab="Rear Width", xlab=" Carapace length")
points(male_data$CL, male_data$RW, col="blue")
points(female_data$CL, female_data$RW, col="red")

legend(35, 10, legend=c("Male crabs", "Female crabs"),
       col=c("blue", "red"), lty=1, cex=0.8)

```

Looking at Figure \ref{fig:crabs}, we see that drawing a line to discriminate between male and female crabs on these data would be very feasable, thus classifying by linear discriminant analysis would be easy in this case.

## 1.2

If we compare Figure \ref{fig:crabs_lda} to Figure \ref{fig:crabs}, we see that they are very similar. There are some differences when carapace length is small, e.g the bottom left point is actually female. but is classified as male with the lda.
There are also some differences when you look at the points which are kind of overlapping each other (some males classfied as females and vice versa).
We notice though, that the bigger the carapace length is, the more accurate the lda model is!



```{r lda_crabs, fig.pos='H', fig.cap="\\label{fig:crabs_lda} Carapace Length vs Rear Width classified by sex using lda", fig.width=5, fig.height=4}
#1.2
lda_crabs = lda(sex ~ CL + RW, data=data1, CV=TRUE)
pred_female_data = data1[lda_crabs$posterior[,1] > lda_crabs$posterior[,2],]
pred_male_data = data1[lda_crabs$posterior[,2] > lda_crabs$posterior[,1],]

plot(data1$CL, data1$RW, ylab="Rear Width", xlab=" Carapace length")
points(pred_male_data$CL, pred_male_data$RW, col="blue")
points(pred_female_data$CL, pred_female_data$RW, col="red")

legend(35, 10, legend=c("Male crabs", "Female crabs"),
       col=c("blue", "red"), lty=1, cex=0.8)

```

```{r lda_miss, eval=FALSE}
pred_sex_list = lda_crabs$posterior[,2] > lda_crabs$posterior[,1] # True is male
sex_list = data1$sex == "Male" # True is male

wrongs = length(data1[sex_list != pred_sex_list, ][,1])

missclassification = wrongs / length(data1[,1])

missclassification

```
When using this lda model we get a missclassification rate of $4.5\%$, this indicates that the model is very well fitted to this problem.

## 1.3

```{r lda_crabs_prior, fig.pos='H', fig.cap="\\label{fig:crabs_lda_prior} Carapace Length vs Rear Width classified by sex using lda (with prior)", fig.width=5, fig.height=4}
#1.3
lda_crabs = lda(sex ~ CL + RW, data=data1, CV=TRUE, prior=c(0.1, 0.9))
pred_female_data = data1[lda_crabs$posterior[,1] > lda_crabs$posterior[,2],]
pred_male_data = data1[lda_crabs$posterior[,2] > lda_crabs$posterior[,1],]

plot(data1$CL, data1$RW, ylab="Rear Width", xlab=" Carapace length")
points(pred_male_data$CL, pred_male_data$RW, col="blue")
points(pred_female_data$CL, pred_female_data$RW, col="red")

legend(35, 10, legend=c("Male crabs", "Female crabs"),
       col=c("blue", "red"), lty=1, cex=0.8)

```

```{r lda_miss_prior, eval=FALSE}
pred_sex_list = lda_crabs$posterior[,2] > lda_crabs$posterior[,1] # True is male
sex_list = data1$sex == "Male" # True is male

wrongs = length(data1[sex_list != pred_sex_list, ][,1])

missclassification = wrongs / length(data1[,1])

missclassification

```

As seen in Figure \ref{fig:crabs_lda_prior},when using $p(Male)=0.9, p(Female)=0.1$ as a prior, we get (compared to Figure \ref{fig:crabs_lda_prior}), as expected when weighting the male sex higher, a more male dominated graph.
This can especially be seen in the lower bounds of carapace length and the values that are pretty close in between the two separation (e.g. the value around $CL=38$ and $RW=15$ is female in Figure \ref{fig:crabs_lda} and male in Figure \ref{fig:crabs_lda_prior}).

If we look at the missclassification rate when using the prior we get a missclassification rate of $8\%$, this is still very good, but almost twice as much when not using prior (or rather using a prior $p(Male)=p(Female)=0.5$)

## 1.4

```{r crab_glm}
#1.4
crab_glm = glm(sex ~ CL + RW, data = data1, family=binomial())
glm_pred = predict(crab_glm, type="response")

male_glm = data1[glm_pred >= 0.5,]
female_glm = data1[glm_pred < 0.5,]

```

```{r crab_glm_plot, fig.pos='H', fig.cap="\\label{fig:crabs_glm} Carapace Length vs Rear Width classified by sex using glm", fig.width=5, fig.height=4}
plot(data1$CL, data1$RW, ylab="Rear Width", xlab=" Carapace length")
points(male_glm$CL, male_glm$RW, col="blue")
points(female_glm$CL, female_glm$RW, col="red")

slope = coef(crab_glm)[2]/(-coef(crab_glm)[3])
intercept = coef(crab_glm)[1]/(-coef(crab_glm)[3]) 
abline(a=intercept, b=slope, col="green")

legend(33, 10, legend=c("Male crabs", "Female crabs", "Decision boundary"),
       col=c("blue", "red", "green"), lty=1, cex=0.8)

```

If we look at Figure \ref{fig:crabs_glm} we see that it looks very similar to all the other figures. It looks very much like Figure \ref{fig:crabs_lda}, and is more keen to classify uncertain values as female than when using lda with prior.

```{r glm_miss}
glm_sex_list = glm_pred > 0.5 # True is male
sex_list = data1$sex == "Male" # True is male

wrongs = length(data1[sex_list != glm_sex_list, ][,1])

missclassification = wrongs / length(data1[,1])

#missclassification
#slope
#intercept

```

Missclassification rate for this method is $3.5\%$, so slightly better than the other methods.

The equation for the decision boundary (green line in Figure \ref{fig:crabs_glm}) is $RW = 0.369 CL + 1.08$

# Assignment 2

# Assignment 4 - Principal components

## 1 Standard PCA
```{r, echo=FALSE}
# 4.1
data = read.csv2("NIRSpectra.csv")
data_cp = data
data_cp$Viscosity = c()
res=prcomp(data_cp)
lambda=res$sdev^2
#eigenvalues
lambda
#proportion of variation
calc = lambda/sum(lambda)*100
sprintf("%2.3f", calc)
sprintf("Feature 1 & 2 gives us a percentage of %2.3f", calc[1]+calc[2])
screeplot(res)

plot(res$x[,1], res$x[,2])

```

In the plot with all the points we can see some outliners that can be viewed as being 'unusual', as the are not part of the sort of big cloud of points.

## 2 Trace plots
```{r, echo=FALSE}
# 4.2
U=res$rotation
plot(U[,1], main="Traceplot, PC1")
plot(U[,2],main="Traceplot, PC2")

```

Yes, PC2 can be explained by mainly a few original features. The features around the peak at index 124 contributes a lot to that component. The range in PC1 is from about 0.08 to 0.110 wich is pretty narrow so no one of the features really stand out.

## 3 Independent Component Analysis
#### A
```{r, echo=FALSE}
# 4.3
set.seed(12345)
ica = fastICA(data_cp, n.comp = 2)
w_prim = ica$K %*% ica$W
plot(w_prim[,1], main="Traceplot, W1")
plot(w_prim[,2], main="Traceplot, W2")

```

The measures representing is the loadings, which we can see as the plots look like the ones in 4.2 only mirrored.

#### B
```{r, echo=FALSE}
plot(ica$X[,1], ica$X[,2])
```

The plot plot above looks linear inverse exponential, where there are a lot of points near 0. This in comparision to the plot in 4.1 where it is sort of a cloud of points where it is harder to see their relative importance to each other.

\pagebreak
# Code appendix

```{r ref.label=knitr::all_labels(), echo = T, eval = F}
```